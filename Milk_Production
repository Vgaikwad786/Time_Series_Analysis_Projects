This file contains Milk_Production data and using TimeSeriesAnalysis
we can calculate the loss and find the hidden patterns in the data aligning the equation
Then compare the data to see the visible results.


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')

sns.set_style('darkgrid')


import tensorflow as tf
GPU = tf.config.list_physical_devices('GPU')
if GPU:
  print('GPU is available and it's working')
else:
  print('CPU is currently working')


df = pd.read_csv('monthly_milk_production.csv',index_col='Date',parse_dates=True)

df.head(5)
df.tail(5)
df.shape
df.info()
df.describe().T

df.plot(figsize=(15,7))

from statsmodels.tsa.seasonal import seasonal_decompose
result = seasonal_decompose(df['Production'])
result.plot()

len(df)

train = df.iloc[:156]
test = df.iloc[156:]

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaled_train = scaler.fit_transform(train)
scaled_test = scaler.transform(test)

scaled_train[:12]

import tensorflow
import keras
from tensorflow.keras.preprocessing.sequence import timeseriesGenerator

# First 3 months data
n_input = 3
n_features = 1
generator = TimeseriesGenerator(scaled_train, scaled_test, length = n_input, batch_size=1)

X, y = generator[0]
print(f'Given array{X.flatten()}')
print(f'Predict{y}')

# Data for first 12 months
n_input = 12
generator = TimeseriesGenerator(scaled_train, scaled_train, length=n_input, batch_size=1)

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM

# define model
# Adam Optimizer is a combination of both Ada-delta and RMS Prop
# 10 Input_layers of LSTM and 1 Hidden state i.e. Dense Layer
# Loss is Mean_squared_error i.e. (actual - predicted)**2
model = Sequential()
model.add(LSTM(100, activation='relu', input_shape=(n_input, n_features)))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')

model.summary()

model.fit(generator, epochs=50)

loss_per_epoch = model.history.history['loss']
plt.plot(range(len(loss_per_epoch)),loss_per_epoch)

last_train_batch = scaled_train[-12:]
last_train_batch = last_train_batch.reshape((1, n_input, n_features))
model.predict(last_train_batch)

scaled_test[0]

test_predictions = []

first_eval_batch = scaled_train[-n_input:]
current_batch = first_eval_batch.reshape((1, n_input, n_features))

for i in range(len(test)):

    # get the prediction value for the first batch
    current_pred = model.predict(current_batch)[0]

    # append the prediction into the array
    test_predictions.append(current_pred)

    # use the prediction to update the batch and remove the first value
    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)

print(test_predictions)
test['Predictions'] = true_predictions

test.plot(figsize=(15,7))


from sklearn.metrics import mean_squared_error
from math import sqrt
rmse=sqrt(mean_squared_error(test['Production'],test['Predictions']))
print(rmse)

''' Hence the final computed loss is 19.39'''
